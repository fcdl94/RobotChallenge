{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from Piggyback.custom_layers import MaskedConv2d\n",
    "\n",
    "\n",
    "# Define a residual block\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, first=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel_size, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if first:\n",
    "            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                                          nn.BatchNorm2d(planes))\n",
    "        self.stride = stride\n",
    "        self.first = first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define a residual block\n",
    "class BasicMaskedBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, first=False, model_size=1):\n",
    "        super(BasicMaskedBlock, self).__init__()\n",
    "        self.conv1 = MaskedConv2d(inplanes, planes, kernel_size=kernel_size, stride=stride, padding=1,\n",
    "                                  bias=False, model_size=model_size)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = MaskedConv2d(planes, planes, kernel_size=kernel_size, padding=1, bias=False, model_size=model_size)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if first:\n",
    "            self.downsample = nn.Sequential(MaskedConv2d(inplanes, planes, kernel_size=1, stride=stride,\n",
    "                                                         bias=False, model_size=model_size),\n",
    "                                            nn.BatchNorm2d(inplanes))\n",
    "        self.stride = stride\n",
    "        self.first = first\n",
    "        self.index = 0\n",
    "\n",
    "    def set_index(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, resnet_block, widening_factor=4, kernel_size=3, classes=[1000]):\n",
    "        super(WideResNet, self).__init__()\n",
    "        \n",
    "        self.block = nn.Conv2d\n",
    "        self.in_channel = 16\n",
    "        \n",
    "        self.conv1 = self.block(3, self.in_channel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer_(resnet_block, 64, kernel_size, widening_factor, stride=2)\n",
    "        self.layer2 = self._make_layer_(resnet_block, 128, kernel_size, widening_factor, stride=2)\n",
    "        self.layer3 = self._make_layer_(resnet_block, 256, kernel_size, widening_factor, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc = nn.ModuleList([nn.Linear(256, c) for c in classes])\n",
    "        self.index = 0\n",
    "        \n",
    "    def set_index(self, index):\n",
    "        if index < len(self.fc):\n",
    "            self.index = index\n",
    "            i = index\n",
    "            for par in self.fc[i].parameters():\n",
    "                par.requires_grad = True\n",
    "\n",
    "            for i in range(0, self.models):\n",
    "                if i != index:\n",
    "                    for par in self.fc[i].parameters():\n",
    "                        par.requires_grad = False\n",
    "                        \n",
    "    def add_task(self, module):\n",
    "        self.fc.append(module)\n",
    "        return len(self.fc) - 1  # return actual index of the added module\n",
    "        \n",
    "    def _make_layer_(self, resnet_block, planes, kernel_size, blocks, stride=1):\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for i in range(0, blocks):\n",
    "            layers.append(resnet_block(self.in_channel, planes, kernel_size, stride=strides[i], first=(i == 0)))\n",
    "            self.in_channel = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc[self.index](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PiggybackNet(nn.Module):\n",
    "    def __init__(self, widening_factor=4, kernel_size=3, classes=[1000]):\n",
    "        super(PiggybackNet, self).__init__()\n",
    "\n",
    "        self.block = MaskedConv2d\n",
    "        self.resnet_block = BasicMaskedBlock\n",
    "        self.in_channel = 16\n",
    "        self.models = len(classes)\n",
    "\n",
    "        self.conv1 = MaskedConv2d(3, self.in_channel, kernel_size=kernel_size, stride=1,\n",
    "                                  padding=1, model_size=self.models)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer_(self.resnet_block, 64, kernel_size, widening_factor, stride=2)\n",
    "        self.layer2 = self._make_layer_(self.resnet_block, 128, kernel_size, widening_factor, stride=2)\n",
    "        self.layer3 = self._make_layer_(self.resnet_block, 256, kernel_size, widening_factor, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fc = nn.ModuleList([nn.Linear(256, c) for c in classes])\n",
    "        self.index = 0\n",
    "\n",
    "    def set_index(self, index):\n",
    "        if index < len(self.fc):\n",
    "            self.index = index\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, MaskedConv2d):\n",
    "                    m.set_index(index)\n",
    "            # self.conv1.set_index(index)\n",
    "            # for mod in self.layer1.modules():\n",
    "            #     if isinstance(mod, MaskedConv2d):\n",
    "            #         mod.set_index(index)\n",
    "            # for mod in self.layer2.modules():\n",
    "            #     if isinstance(mod, MaskedConv2d):\n",
    "            #         mod.set_index(index)\n",
    "            # for mod in self.layer3.modules():\n",
    "            #     if isinstance(mod, MaskedConv2d):\n",
    "            #         mod.set_index(index)\n",
    "            # set to true correct modules\n",
    "            i = index\n",
    "            for par in self.fc[i].parameters():\n",
    "                par.requires_grad = True\n",
    "            # set to false others\n",
    "            for i in range(0, self.models):\n",
    "                if i != index:\n",
    "                    for par in self.fc[i].parameters():\n",
    "                        par.requires_grad = False\n",
    "\n",
    "    def add_task(self, module):\n",
    "        self.fc.append(module)\n",
    "        return len(self.fc) - 1  # return actual index of the added module\n",
    "\n",
    "    def _make_layer_(self, block, planes, kernel_size, blocks, stride=1):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(0, blocks):\n",
    "            layers.append(block(self.in_channel, planes, kernel_size,\n",
    "                                stride=strides[i], first=(i == 0), model_size=self.models))\n",
    "            self.in_channel = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc[self.index](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def wide_resnet(model_classes, pretrained=None, frozen=False, imagenet_old=False):\n",
    "    model = WideResNet(BasicBlock, classes=model_classes)\n",
    "    if pretrained:\n",
    "        old_state = torch.load(pretrained)['state_dict']\n",
    "        state = model.state_dict()\n",
    "        state.update(old_state)\n",
    "        model.load_state_dict(state, False)\n",
    "\n",
    "        if imagenet_old:\n",
    "            dict_fc = dict(model.fc.named_parameters())\n",
    "            dict_fc[\"0.weight\"].data.copy_(old_state[\"fc.weight\"].data)\n",
    "            dict_fc[\"0.bias\"].data.copy_(old_state[\"fc.bias\"].data)\n",
    "\n",
    "        print(\"Model pretrained loaded\")\n",
    "\n",
    "    if frozen:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"fc\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def piggyback_net(model_classes, pretrained=None, imagenet_old=False, bn=False):\n",
    "    model = PiggybackNet(classes=model_classes)\n",
    "    if pretrained:\n",
    "        old_state = torch.load(pretrained)['state_dict']\n",
    "        state = model.state_dict()\n",
    "        state.update(old_state)\n",
    "        model.load_state_dict(state, False)\n",
    "\n",
    "        if imagenet_old:\n",
    "            dict_fc = dict(model.fc.named_parameters())\n",
    "            dict_fc[\"0.weight\"].data.copy_(old_state[\"fc.weight\"].data)\n",
    "            dict_fc[\"0.bias\"].data.copy_(old_state[\"fc.bias\"].data)\n",
    "\n",
    "        if not bn:\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.requires_grad = False\n",
    "                    m.bias.requires_grad = False\n",
    "        \n",
    "        print(\"Model pretrained loaded\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network\n",
    "class ResNet28(nn.Module):\n",
    "    def __init__(self, block, n_size, init=0.01, classes=1000, masked=False, quantized=0, bn=False):\n",
    "        super(ResNet28, self).__init__()\n",
    "        self.inplane = 16\n",
    "\n",
    "        if masked:\n",
    "            self.conv1 = custom_layers.MaskedConv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.inplane)\n",
    "        elif quantized:\n",
    "            self.conv1 = custom_layers.QuantizedConv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False,\n",
    "                                                       masks=1)\n",
    "            self.bn1 = nn.BatchNorm2d(self.inplane)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.inplane)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks=n_size, stride=2, )\n",
    "        self.layer2 = self._make_layer(block, 128, blocks=n_size, stride=2,)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks=n_size, stride=2,)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fc = nn.Linear(256, classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride, bn=False):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for i in range(len(strides)):\n",
    "            layers.append(\n",
    "                block(self.inplane, planes, strides[i], first=(i == 0)))\n",
    "            self.inplane = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize standard ResNet\n",
    "def resnet28(pretrained=None, classes=1000, frozen=False):\n",
    "    model = ResNet28(BasicBlock, 4, classes=classes)\n",
    "    if pretrained:\n",
    "        state = model.state_dict()\n",
    "        state.update(torch.load(pretrained)['state_dict'])\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "    if frozen == 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"fc\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = piggyback_net([51,18,10])\n",
    "\n",
    "rm = resnet28()\n",
    "# state = m.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PiggybackNet:\n\tWhile copying the parameter named \"layer1.0.conv1.weight\", whose dimensions in the model are torch.Size([64, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 16, 2, 2]).\n\tWhile copying the parameter named \"layer1.0.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 2, 2]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.weight\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.bias\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer2.0.conv1.weight\", whose dimensions in the model are torch.Size([128, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 64, 2, 2]).\n\tWhile copying the parameter named \"layer2.0.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 2, 2]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.weight\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.bias\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer3.0.conv1.weight\", whose dimensions in the model are torch.Size([256, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 128, 2, 2]).\n\tWhile copying the parameter named \"layer3.0.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 2, 2]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.weight\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.bias\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-43039b5e4a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PiggybackNet:\n\tWhile copying the parameter named \"layer1.0.conv1.weight\", whose dimensions in the model are torch.Size([64, 16, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 16, 2, 2]).\n\tWhile copying the parameter named \"layer1.0.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 2, 2]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.weight\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.bias\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer1.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([16]) and whose dimensions in the checkpoint are torch.Size([64]).\n\tWhile copying the parameter named \"layer2.0.conv1.weight\", whose dimensions in the model are torch.Size([128, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 64, 2, 2]).\n\tWhile copying the parameter named \"layer2.0.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 2, 2]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.weight\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.bias\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer2.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([64]) and whose dimensions in the checkpoint are torch.Size([128]).\n\tWhile copying the parameter named \"layer3.0.conv1.weight\", whose dimensions in the model are torch.Size([256, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 128, 2, 2]).\n\tWhile copying the parameter named \"layer3.0.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 2, 2]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.weight\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.bias\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.running_mean\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256]).\n\tWhile copying the parameter named \"layer3.0.downsample.1.running_var\", whose dimensions in the model are torch.Size([128]) and whose dimensions in the checkpoint are torch.Size([256])."
     ]
    }
   ],
   "source": [
    "old_state = rm.state_dict()\n",
    "state = pm.state_dict()\n",
    "state.update(old_state)\n",
    "pm.load_state_dict(state, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
